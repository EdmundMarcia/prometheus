---
layout: default
---

How can you evaluate whether your LLM is humorous or not? Among various version during development, how can you track whether your LLM is inspiring while being culturally sensitive?

The current evaluation resources such as MMLU, Big Bench, AlpacaFarm are confined to generic, single-dimensional evaluation metrics that are either too domain / task specific (e.g., EM, Rouge) or coarse-grained (e.g., helpfulness/harmlessness). While VicunaBench and MT Bench offers instructions to evaluate different abilities (e.g., role playing, writing, coding), the current evaluation paradigm heavily relies on GPT-4 evaluation, which has the following disadvantages:
* Close-source Nature: The proprietary nature of LLMs brings transparency concerns as internal workings are not disclosed to the broader academic community.
* Uncontrolled Versioning: As reproducibility is a cornerstone of scientific inquiry, any inconsistency stemming from version changes can undermine the robustness of research findings that depend on specific versions of the evaluator LM.
* Prohibitive Costs: Evaluating 4 LLM variants across four sizes (ranging from 7B to 65B) using GPT-4 on 1K evaluation instances can cost over $2000.

<span class="sys-name">**Prometheus**</span> ðŸ”¥ is a fully open-source LLM (7B & 13B) that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. Along with the model, we release the **Feedback Collection**, which is a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4.


<br/>

{: .sys-img}
![Animation of the overall workflow of EvalLM where users sample inputs from a dataset, generate outputs from each input using two different prompts, and then comparatively evaluate these outputs on user-defined criteria.](/assets/img/animation.gif)

------

## Interface

The main screen of the interface consists of three panels.

{: .sys-img}
![Main screen of EvalLM shows three panels. The generation panel shows text boxes for the prompt and task instruction, and buttons for input sampling. The evaluation panel shows text boxes for the criteria, buttons for evaluating, and stacked bar charts for the evaluation results.](/assets/img/figure1.png)

<b>Generation Panel</b>: To generate outputs, the user defines their overall **task instruction** (A), two **prompts** they want to compare (B), and then **samples inputs** from a dataset (C) which will be used to test the prompts.

**Evaluation Panel**: To evaluate outputs, the user defines a set of evaluation **<a href="#criteria" target="_self">criteria</a>** (D). Then, after evaluating, they can verify the overall *evaluation* performance of each prompt (E) or, if they created a validation set, *validate* how automatic evaluations align with ground-truth evaluations (F).

**Data Panel**: This panel shows **<a href="#datarow" target="_self">data rows</a>** containing inputs, outputs, and evaluation results. 

<br/>

### <span id="motivation">Motivation</span>

{: .text-left}
<span class="sys-name">EvalLM</span> allows users to evaluate outputs on their own criteria specific to their application and/or context. 
<br/><br/>
To define a criteria, the user simply provides the criteria with a **name** (A) and **description** (B) in natural language.
<br/><br/>
To assist users in creating more effective and helpful criteria, the system automatically **reviews** their criteria (C) and provides **suggestions** (D) on how the criteria can be *refined*, *merged* and *split*.

{: .img-right}
![Criteria are represented as a set of text boxes that contain the name and description of the criteria. Suggested revisions are shown below the criteria.](/assets/img/figure2.png)

<br/>

### <span id="datarow">Data Row</span>

{: .sys-img}
![Data Rows in the interface display inputs, output pairs, and evaluation results. Clicking on evaluation results opens a panel that shows the explanation for that evaluation underneath the row.](/assets/img/figure2.png)

For each sampled **input** (A), the interface presents the **outputs** generated from each prompt side-by-side (B) and the **evaluation results** for each criteria next to the outputs (C). For each criteria, the evaluation results show which prompt produced the output that better satisfied that criteria.

If the user wants to see more details, they can click on one of these evaluations to see the assistant's **explanation** (D). To help the user match the explanation and outputs, the system also **highlights** spans from the outputs that were considered to be important when evaluating the criteria (E).

If the user selected to evaluate outputs on multiple trials, they can see the evaluations for **other trials** through the carousel (F). 

------

## Bibtex
<pre>
@misc{kim2023prometheus,
      title={Prometheus: Inducing Fine-grained Evaluation Capability in Language Models}, 
      author={Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
      year={2023},
      eprint={2310.08491},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
</pre>

------

{: .logos}
[![Logo of LKLab](/assets/img/lklab_logo.png)](https://lklab.kaist.ac.kr/)
[![Logo of KAIST](/assets/img/kaist_logo.png)](https://kaist.ac.kr)
[![Logo of NAVER](/assets/img/naver_ai_lab_logo.png)](https://www.facebook.com/NAVERAILAB)

{: .center .acknowledgement}
This research was supported by the **KAIST-NAVER Hypercreative AI Center**.
